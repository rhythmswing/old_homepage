<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2018-04-16T20:24:07+08:00</updated><id>/</id><title type="html">Rui’s</title><entry><title type="html">Pointwise Mutual Information and Word Embedding, and Graph Representation Learning</title><link href="/representation/learning/2018/04/16/Pointwise-Mutual-Information-and-Word-Embedding-and-Graph-Representation-Learning.html" rel="alternate" type="text/html" title="Pointwise Mutual Information and Word Embedding, and Graph Representation Learning" /><published>2018-04-16T07:29:54+08:00</published><updated>2018-04-16T07:29:54+08:00</updated><id>/representation/learning/2018/04/16/Pointwise-Mutual-Information-and-Word-Embedding-and-Graph-Representation-Learning</id><content type="html" xml:base="/representation/learning/2018/04/16/Pointwise-Mutual-Information-and-Word-Embedding-and-Graph-Representation-Learning.html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;One important question in machine learning is to represent effectively complex objects in computationally tractable approach. Common procedures are to assign these objects some vectors in a finite dimensional vector space, taking advantage of the geometric structure of the vector space.&lt;/p&gt;

&lt;p&gt;​	Examples include: words, sentences, documents in NLP; vertices in graphs; images; etc… In machine learning, people call it “embedding”: word embedding, graph embedding. We call such assignment of vectors is called “embedding”. Formally, given a set of objects $\mathcal{O}$, the embedding is a mapping $\Phi:\mathcal{O}\rightarrow \mathcal{R}^d$.&lt;/p&gt;

&lt;p&gt;​	This is a summary of two representation algorithms: word2vec, which learns word embedding, and Deep2walk, which learns embedding for vertices in a social network.&lt;/p&gt;

&lt;h1 id=&quot;word2vec-and-pointwise-mutual-information&quot;&gt;Word2vec and Pointwise Mutual Information&lt;/h1&gt;

&lt;p&gt;​	For details of word2vec, please refer to &lt;a href=&quot;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&quot;&gt;this tutorial&lt;/a&gt;. We analyze the skip-gram model which is recommended by Mikolov et al., the authors of the original paper of word2vec.&lt;/p&gt;

&lt;p&gt;​	Suppose the vocabulary is $V$. Each word in $V$ has a dual identity: it can be a word per se, or the &lt;em&gt;context&lt;/em&gt; of some other word. We denote the set of words by $V_W$ and context by $V_C$. A popular approach of sampling word-context pairs is to define the context of $w$ as words which appear surrounding $w$; we denote the collection of words and context pairs as $D$.&lt;/p&gt;

&lt;p&gt;​	Word2vec gives each word two vector representations: one is for the word per se, and the other for the word as a context. That is to say, for each word $w\in V_W$, its representation is $\mathbf{w}\in\mathcal{R}^d$, and for $c\in V_C$, we have $\mathbf{c}\in\mathbf{R}^d$.&lt;/p&gt;

&lt;p&gt;​	To learn the embedding, word2vec parameterized the occurrence of a word-context pair: &lt;script type=&quot;math/tex&quot;&gt;P((w,c)\in D) = \sigma(\mathbf{w}^T\mathbf{c})&lt;/script&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Introduction One important question in machine learning is to represent effectively complex objects in computationally tractable approach. Common procedures are to assign these objects some vectors in a finite dimensional vector space, taking advantage of the geometric structure of the vector space. ​ Examples include: words, sentences, documents in NLP; vertices in graphs; images; etc… In machine learning, people call it “embedding”: word embedding, graph embedding. We call such assignment of vectors is called “embedding”. Formally, given a set of objects $\mathcal{O}$, the embedding is a mapping $\Phi:\mathcal{O}\rightarrow \mathcal{R}^d$. ​ This is a summary of two representation algorithms: word2vec, which learns word embedding, and Deep2walk, which learns embedding for vertices in a social network. Word2vec and Pointwise Mutual Information ​ For details of word2vec, please refer to this tutorial. We analyze the skip-gram model which is recommended by Mikolov et al., the authors of the original paper of word2vec. ​ Suppose the vocabulary is $V$. Each word in $V$ has a dual identity: it can be a word per se, or the context of some other word. We denote the set of words by $V_W$ and context by $V_C$. A popular approach of sampling word-context pairs is to define the context of $w$ as words which appear surrounding $w$; we denote the collection of words and context pairs as $D$. ​ Word2vec gives each word two vector representations: one is for the word per se, and the other for the word as a context. That is to say, for each word $w\in V_W$, its representation is $\mathbf{w}\in\mathcal{R}^d$, and for $c\in V_C$, we have $\mathbf{c}\in\mathbf{R}^d$. ​ To learn the embedding, word2vec parameterized the occurrence of a word-context pair:</summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="/jekyll/update/2018/03/22/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2018-03-22T07:29:54+08:00</published><updated>2018-03-22T07:29:54+08:00</updated><id>/jekyll/update/2018/03/22/welcome-to-jekyll</id><content type="html" xml:base="/jekyll/update/2018/03/22/welcome-to-jekyll.html">&lt;p&gt;Hello, world!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pics/scalefree_illustrate.png&quot; alt=&quot;Illustration of Scale-free&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello, world!</summary></entry></feed>